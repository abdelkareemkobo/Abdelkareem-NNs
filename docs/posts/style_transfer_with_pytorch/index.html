<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Abdelkareem Elkhatib">

<title>Abdelkareem NNs - Style_transfer_with_pytorch</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Abdelkareem NNs</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html">Blog</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/abdelkareemkobo"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/Abdelakreemkobo"><i class="bi bi-twitter" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-color-scheme-toggle nav-link" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Style_transfer_with_pytorch</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Abdelkareem Elkhatib </p>
            </div>
    </div>
      
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Tabel of contents</h2>
   
  <ul>
  <li><a href="#style-transfer" id="toc-style-transfer" class="nav-link active" data-scroll-target="#style-transfer">Style Transfer</a>
  <ul class="collapse">
  <li><a href="#how-does-it-work" id="toc-how-does-it-work" class="nav-link" data-scroll-target="#how-does-it-work">How does it work?</a></li>
  </ul></li>
  <li><a href="#loading-the-model-the-vgg19" id="toc-loading-the-model-the-vgg19" class="nav-link" data-scroll-target="#loading-the-model-the-vgg19">Loading the model (The VGG19 )</a>
  <ul class="collapse">
  <li><a href="#extracting-the-features" id="toc-extracting-the-features" class="nav-link" data-scroll-target="#extracting-the-features">Extracting the features</a></li>
  <li><a href="#the-gram-matrix" id="toc-the-gram-matrix" class="nav-link" data-scroll-target="#the-gram-matrix">The gram matrix</a></li>
  </ul></li>
  <li><a href="#setting-up-the-feature-extraction-process" id="toc-setting-up-the-feature-extraction-process" class="nav-link" data-scroll-target="#setting-up-the-feature-extraction-process">Setting up the Feature Extraction process</a></li>
  <li><a href="#optimization-algorithmlossesand-parameters-updates" id="toc-optimization-algorithmlossesand-parameters-updates" class="nav-link" data-scroll-target="#optimization-algorithmlossesand-parameters-updates">Optimization Algorithm,Losses,and Parameters Updates</a></li>
  </ul>
</nav>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="style-transfer" class="level1">
<h1>Style Transfer</h1>
<p>in simple words, style transfer consists of modifying the style of an image,whilte still preserving its content. for instance taking an image of an animal and transforming the style into a van Goh like painting.</p>
<section id="how-does-it-work" class="level2">
<h2 class="anchored" data-anchor-id="how-does-it-work">How does it work?</h2>
<ol type="1">
<li><p>feeding the inputs: Both the content and the style image are fed to the model,and they need to be the same. A common practice here is to resize the style image to be the same shape of the content image</p></li>
<li><p>Loading the model: try model like VGG network,which performs outstandingly well over style transfer problems.</p></li>
<li><p>Determining the layers’functions: Given that there are two main tasks at hand (recognizing the content of an image and distinguishing the style of another one ),different layers will have different functions to extract the different features.for the style image ,the focus should be on colors and textures,while for the content image,the focus should be on edges and forms. in this step ,the different layers are separated into different tasks.</p></li>
<li><p>Defining the optimization problem: Unlike other supervised problems. it is required to minimize three different loss functions for style transfer problems:</p></li>
</ol>
<ul>
<li><p>Content loss: this measures the distance between the content image and the output,only considering features related to content</p></li>
<li><p>Style loss: this measures the distance between the style image and the output only considering features related to style</p></li>
<li><p>Total loss: This combines both the content and style loss. Both the content and style loss have a weight associated to them, which is used to determine their participation in the calculation of the total loss.</p></li>
</ul>
<ol start="5" type="1">
<li>Parameter update: This step uses gradients to update the different parameters of the network</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np <span class="co">## this will be used to transform images to be displayed</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch </span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn, optim <span class="co">#These will implement the neural network as we as define the optimization algorithm </span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image <span class="co"># This will load images </span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt <span class="co"># this will display images </span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> transforms, models <span class="co"># These will convert the images into tensors and load the pretrained model </span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">'cuda'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>imsize <span class="op">=</span> <span class="dv">224</span> <span class="co"># set the image size  for both images </span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># set the transformations to be performed over images --&gt; resize--&gt; converting to tensors --&gt; normalizing </span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>loader <span class="op">=</span> transforms.Compose([</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    transforms.Resize(imsize), </span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize((<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>), (<span class="fl">0.229</span>, <span class="fl">0.224</span>, <span class="fl">0.225</span>))])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note:The VGG network was trained using normalized images, where each channel has a mean of 0.485, 0.456, and 0.406, respectively, and a standard deviation of 0.229, 0.224, and 0.225.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> image_loader(image_name):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Function to receive the image path as input and use PIL to </span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co">    open the image and apply transformations over the image </span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> Image.<span class="bu">open</span>(image_name)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> loader(image).unsqueeze(<span class="dv">0</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> image</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>content_img <span class="op">=</span> image_loader(<span class="st">"images/landscape.jpg"</span>).to(device)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>style_img <span class="op">=</span> image_loader(<span class="st">"images/monet.jpg"</span>).to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li><p>To display the images,convert them back to PIL images and revert the normalization process, Define these transformation in a variable</p></li>
<li><p>Note: To revert the normalization, it is necessary to use as mean the negative value of the mean used for normalizing the data, divided by the standard deviation previously used for normalizing the data. Moreover, the new standard deviation should be equal to one divided by the standard deviation used to normalize the data before.</p></li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>unloader <span class="op">=</span> transforms.Compose([</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize((<span class="op">-</span><span class="fl">0.485</span><span class="op">/</span><span class="fl">0.229</span>, <span class="op">-</span><span class="fl">0.456</span><span class="op">/</span><span class="fl">0.224</span>, <span class="op">-</span><span class="fl">0.406</span><span class="op">/</span><span class="fl">0.225</span>), (<span class="dv">1</span><span class="op">/</span><span class="fl">0.229</span>, <span class="dv">1</span><span class="op">/</span><span class="fl">0.224</span>, <span class="dv">1</span><span class="op">/</span><span class="fl">0.225</span>)),</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    transforms.ToPILImage()])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tensor2image(tensor):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> tensor.to(<span class="st">'cpu'</span>).clone() </span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> image.squeeze(<span class="dv">0</span>)  </span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> unloader(image)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> image</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>plt.imshow(tensor2image(content_img))</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Content Image"</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>plt.imshow(tensor2image(style_img))</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Style Image"</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="loading-the-model-the-vgg19" class="level1">
<h1>Loading the model (The VGG19 )</h1>
<p>The saved model in Pytorch is split into two portions:</p>
<ol type="1">
<li><p>vgg19.features: this consists of all the convolutional and pooling layers of the network along with the parameters.These layers are in charge of extracting the features from images,where some of the layers specialize in style features,such as colors, while other specialize in content features,such as edges.</p></li>
<li><p>vgg19.classifier: This refers to the linear layers(also now as fully connected layers) thar are located at the end of the network,including their parameters. These layers are the ones that perform the classificatoin of the image into one of the label classes ****</p></li>
</ol>
<ul>
<li><p>According to the preceding information, only the features portion of the model should be loaded in order to extract the necessary features of both the content and style images.</p></li>
<li><p>Moreover, the parameters in each layer should be kept unchanged,considering that those are the ones that will help detect the desired features. This can be achieved by defining that the model doesn’t need to calculate gradients for any of these layers.</p></li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> models.vgg19(pretrained<span class="op">=</span><span class="va">True</span>).features</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> param <span class="kw">in</span> model.parameters():</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    param.requires_grad_(<span class="va">False</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>model.to(device)    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="extracting-the-features" class="level2">
<h2 class="anchored" data-anchor-id="extracting-the-features">Extracting the features</h2>
<ul>
<li><p>In the field of style transfer, there have been different papers that have identified those layers that are crucial at recognizing relevant features over the content and style images.</p></li>
<li><p>According to this, it is conventionally accepted that the first convolutional layer of every stack is capable of extracting style features, while only the second convolutional layer of the fourth stack should be used to extract content features.</p></li>
<li><p>From now on, we will refer to the layers that extract the style features as conv1_1, conv2_1,conv3_1, conv4_1, and conv5_1, while the layer in charge of extracting the content features will be known as conv4_2.</p></li>
<li><p>this means that the sytle image should be passed through five different layers. while the content image only needs to go through one layer.</p></li>
<li><p>the output from each of these layers is used to compare the output image to the input images,where the objective would be to modify the parameters of the target image to resemble the content of the content image and the style of the style image,which can be achieved through the optimization of three different loss functions.</p></li>
<li><p>to check the style representation of the target image and the style image, it is necessary to check for correlations and not the strict presence of the features on both images. this is because the style features of both images will not be exact,but rather an approximation.</p></li>
</ul>
</section>
<section id="the-gram-matrix" class="level2">
<h2 class="anchored" data-anchor-id="the-gram-matrix">The gram matrix</h2>
<p>[[https://www.youtube.com/watch?v=Elxnzxk-AUk]]</p>
</section>
</section>
<section id="setting-up-the-feature-extraction-process" class="level1">
<h1>Setting up the Feature Extraction process</h1>
<ul>
<li>create a dictionary mapping the index of the relevant layers(keys) to a name(values).This will facilitate the process of calling relevant layers in the future:</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>relevant_layers <span class="op">=</span> {<span class="st">'0'</span>: <span class="st">'conv1_1'</span>, <span class="st">'5'</span>: <span class="st">'conv2_1'</span>, <span class="st">'10'</span>: <span class="st">'conv3_1'</span>, <span class="st">'19'</span>: <span class="st">'conv4_1'</span>, <span class="st">'21'</span>: <span class="st">'conv4_2'</span>, <span class="st">'28'</span>: <span class="st">'conv5_1'</span>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>create a function that will extract the relevant features(features extracted from the relevant layers only) from an input image.</li>
<li>Name if feature_extractor and make sure it takes as input the image,the model,the model ,and the dictionary previously created</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> features_extractor(x, model, layers):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    features <span class="op">=</span> {}</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> index, layer <span class="kw">in</span> model._modules.items():</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> index <span class="kw">in</span> layers:</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layer(x)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>            features[layers[index]] <span class="op">=</span> x</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> features</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li><p>model._modules contains a dictionary holding each layer of the network. By</p></li>
<li><p>performing a for loop through the different layers, we pass the image through the layers of interest (the ones inside the layers dictionary previously created) and save the output into the features dictionary.</p></li>
<li><p>The output dictionary consists of keys containing the name of the layer and values containing the output features from that layer.</p></li>
<li><p>call the features_extractor function over the content and style images</p></li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>content_features <span class="op">=</span> features_extractor(content_img, model, relevant_layers)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>style_features <span class="op">=</span> features_extractor(style_img, model, relevant_layers)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>Perfrom the gram matrix calculation over style features. Consider that the style features were obtained from different layers, which is why different gram matrices should be created,on of each layer’s output</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>style_grams <span class="op">=</span> {}</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> style_features:</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    layer <span class="op">=</span> style_features[i]</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    _, d1, d2, d3 <span class="op">=</span> layer.shape</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    features <span class="op">=</span> layer.view(d1, d2 <span class="op">*</span> d3)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    gram <span class="op">=</span> torch.mm(features, features.t())</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    style_grams[i] <span class="op">=</span> gram</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>create an initial target image.this image will be later compared against the content and style images and be changed until the desired similarity is achieved:</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>target_img <span class="op">=</span> content_img.clone().requires_grad_(<span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>plt.imshow(tensor2image(target_img))</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Target Image"</span>)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="optimization-algorithmlossesand-parameters-updates" class="level1">
<h1>Optimization Algorithm,Losses,and Parameters Updates</h1>
<ul>
<li><p>Although style transfer is performed using a pretrained network where the parameters are left unchanged,creating the target image consists of an iterative process where three different loss functions are calculated and minimized by updating only the parameters related to the target image.</p></li>
<li><p>However considering that measuring accuracy in terms of content and style is achieved vefy differently,the following is an explanation of the calculation of both the content and style loss functions,as well as a description on how the total loss is calculated</p></li>
</ul>
<section id="content-loss" class="level4">
<h4 class="anchored" data-anchor-id="content-loss">content loss</h4>
<ul>
<li><p>This consists of a function that, based on the feature map obtained by a given layer,calculates the distance between the content image and the target image. In the case ofthe VGG-19 network, the content loss is only calculated based on the output from theconv4_2 layer.</p></li>
<li><p>The main idea behind the content loss function is to minimize the distance between the content image and the target image so that the latter highly resembles the former one in terms of content.</p></li>
<li><p>The content loss can be calculated as the mean squared difference between the feature maps of the content and target images at the relevant layer (conv4_2), which can be achieved using the following equation:</p></li>
</ul>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>content loss <span class="op">=</span> torch.mean((target features <span class="op">-</span> content features)<span class="op">**</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="style-loss" class="level4">
<h4 class="anchored" data-anchor-id="style-loss">Style loss</h4>
<ul>
<li><p>Similar to the content loss, the style loss is a function that measures the distance between the style and the target image in terms of style features (for instance, color and texture) by calculating the mean squared difference. Contrary to the content loss, instead of comparing the feature maps derived from the different layers, it compares the gram matrices calculated based on the feature maps of both the style and the target image.</p></li>
<li><p>It is important to mention that the style loss has to be calculated for all relevant layers (in this case, five layers) using a for loop. This will result in a loss function that considers simple and complex style representations from both images. Furthermore, it is a good practice to weigh the style representation of each of these layers between zero to one in order to give more emphasis to the layers that extract larger and simpler features over layers that extract very complex features. This is achieved by giving higher weights to earlier layers (conv1_1 and conv2_1) that extract more generic features from the style image Considering this, the calculation of the style loss can be performed using the following equation for each of the relevant layers:</p></li>
</ul>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>style loss <span class="op">=</span> style layer weight <span class="op">*</span> torch.mean((target gram <span class="op">-</span> style gram )<span class="op">**</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="total-loss" class="level4">
<h4 class="anchored" data-anchor-id="total-loss">Total loss</h4>
<ul>
<li><p>finally,the total loss function consists of a combination Finally, the total loss function consists of a combination of both the content loss and the style loss.</p></li>
<li><p>Its value is minimized during the iterative process of creating the target image by updating parameters of the target image. Again, it is recommended to assign weights to the content and the style losses in order to determine their participation in the final output.</p></li>
<li><p>This helps determine the degree at which the target image will be stylized, while making the content still visible. Considering this, it is a good practice to set the weight of the content loss as equal to one, whereas the one for the style loss must be much higher to achieve the ratio of your preference.</p></li>
<li><p>The weight assigned to the content loss is conventionally known as alpha, while the one given to the style loss is known as beta. The final equation to calculate the total loss can be seen as follows:</p></li>
</ul>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>total loss <span class="op">=</span> content loss<span class="op">*</span>alpha  <span class="op">+</span> style loss <span class="op">*</span> beta </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><p>Once the weights of the losses are defined, it is time to set the number of iteration steps, as well as the optimization algorithm which should only affect the target image.</p></li>
<li><p>This means that, in every iteration step, all three losses will be calculated to then use the gradients to optimize the parameters associated to the target image, until the loss functions are minimized and a target function with the desired look is achieved.</p></li>
<li><p>Like the optimization of previous neural networks, the following are the steps followed in each iteration:</p></li>
</ul>
<ol type="1">
<li><p>Get the features, both in terms of content and style, from the target image. In the initial iteration, this image will be an exact copy of the content image.</p></li>
<li><p>Calculate the content loss. This is done comparing the content features map of the content and the target images.</p></li>
<li><p>Calculate the average style loss of all relevant layers. This is achieved by comparing the gram matrices for all layers of both the style and target image.</p></li>
<li><p>Calculate the total loss.</p></li>
<li><p>Calculate the partial derivatives of the total loss function in respect to the parameters (weights and biases) of the target image.</p></li>
<li><p>Repeat until the desired number of iterations has been reached. The final output will be an image with content similar to the content image and a style similar to the style image</p></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>style_weights <span class="op">=</span> {<span class="st">'conv1_1'</span>: <span class="fl">1.</span>, <span class="st">'conv2_1'</span>: <span class="fl">0.8</span>, <span class="st">'conv3_1'</span>: <span class="fl">0.6</span>, <span class="st">'conv4_1'</span>: <span class="fl">0.4</span>, <span class="st">'conv5_1'</span>: <span class="fl">0.2</span>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># define the weights associated with the content and style losses.</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>beta <span class="op">=</span> <span class="fl">1e6</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="define-the-number-of-iteration-stepsas-well-as-the-optimization.-we-can-alsot-set-the-number-of-iterations-after-we-want-to-see-a-plot-of-the-image-that-has-been-created-to-that-point.-noteto-reach-an-outstanding-result-in-style-transfer-even-more-iterations-are-typically-requiredaround-6000-perhaps" class="level4">
<h4 class="anchored" data-anchor-id="define-the-number-of-iteration-stepsas-well-as-the-optimization.-we-can-alsot-set-the-number-of-iterations-after-we-want-to-see-a-plot-of-the-image-that-has-been-created-to-that-point.-noteto-reach-an-outstanding-result-in-style-transfer-even-more-iterations-are-typically-requiredaround-6000-perhaps">Define the number of iteration steps,as well as the optimization. We can alsot set the number of iterations after we want to see a plot of the image that has been created to that point. Note:to reach an outstanding result in style transfer even more iterations are typically required(around 6,000 perhaps)</h4>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>print_statement <span class="op">=</span> <span class="dv">5000</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam([target_img], lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>iterations <span class="op">=</span> <span class="dv">30000</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, iterations<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    target_features <span class="op">=</span> features_extractor(target_img, model, relevant_layers)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    content_loss <span class="op">=</span> torch.mean((target_features[<span class="st">'conv4_2'</span>] <span class="op">-</span> content_features[<span class="st">'conv4_2'</span>])<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    style_losses <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> layer <span class="kw">in</span> style_weights:</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>        target_feature <span class="op">=</span> target_features[layer]</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>        _, d1, d2, d3 <span class="op">=</span> target_feature.shape</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>        target_reshaped <span class="op">=</span> target_feature.view(d1, d2 <span class="op">*</span> d3)</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>        target_gram <span class="op">=</span> torch.mm(target_reshaped, target_reshaped.t())</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>        style_gram <span class="op">=</span> style_grams[layer]</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>        style_loss <span class="op">=</span> style_weights[layer] <span class="op">*</span> torch.mean((target_gram <span class="op">-</span> style_gram)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>        style_losses <span class="op">+=</span> style_loss <span class="op">/</span> (d1 <span class="op">*</span> d2 <span class="op">*</span> d3)</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>    total_loss <span class="op">=</span> alpha <span class="op">*</span> content_loss <span class="op">+</span> beta <span class="op">*</span> style_loss</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>    total_loss.backward()</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>  i <span class="op">%</span> print_statement <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> i <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">'Total loss: '</span>, total_loss.item())</span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>        plt.imshow(tensor2image(target_img))</span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>        plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>#plot both the content and the target image to compare the results</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>ax1.imshow(tensor2image(content_img))</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>ax2.imshow(tensor2image(target_img))</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note The paper used as guide for this part can be accessed in the following URL: https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_ Image_Style_Transfer_CVPR_2016_paper.pdf.</p>


</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section></div></main> <!-- /main -->
<div>
<hr>


<h3> Support my work with a coffee </h3>

<script type="text/javascript" src="https://cdnjs.buymeacoffee.com/1.0.0/button.prod.min.js" data-name="bmc-button" data-slug="kareem0109t" data-color="#06436e" data-emoji="☕" data-font="Lato" data-text="Support me" data-outline-color="#ffffff" data-font-color="#ffffff" data-coffee-color="#FFDD00" data-height="40px"></script>

<h3> Share </h3>

<a href="https://twitter.com/share?ref_src=twsrc%5Etfw" class="twitter-share-button" data-size="large" data-hashtags="#rstats" data-show-count="false">Tweet</a><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<hr>
</div>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } 
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="abdelkareemkobo/comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>