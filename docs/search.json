[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "I am an experiential learner who dreams, designs and builds exceptional, high-quality digital products and services. I also train machines to teach themselves."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Abdelkareem NNs",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncnn\n\n\ndeep_learning\n\n\ncomputer_vision\n\n\npytorch\n\n\n\n\n\n\n\n\n\n\n\nOct 8, 2022\n\n\n9 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/buildcnn/index.html",
    "href": "posts/buildcnn/index.html",
    "title": "Building an CNN with Pytorch",
    "section": "",
    "text": "Data Augmentation\nIn simple words, it is a measure to increase the number of training examples by slightly modifying the existing examples. For example, you could duplicate the instances currently available and add some noise to those duplicates to make sure they are not exactly the same. In computer vision problems, this means incrementing the number of images in the training dataset by altering the existing images, which can be done by slightly altering the current images to create duplicated versions that are slightly different These minor adjustments to the images can be in the form of slight rotations, changes in the position of the object in the frame, horizontal or vertical flips, different color schemes, and distortions, among others. This technique works considering that CNNs will consider each of these images a different image.\n\nimport numpy as np\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\n\n\ntransform = {\n    \"train\": transforms.Compose([\n        transforms.RandomHorizontalFlip(0.5), \n        transforms.RandomGrayscale(0.1),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n    \"test\": transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])}\n\n\nConvert the pixels into tensor datatype and normalizing the data\n\nbatch_size = 100\n\ntrain_data = datasets.CIFAR10('data3', train=True, download=True, transform=transform[\"train\"])\ntest_data = datasets.CIFAR10('data3', train=False, download=True, transform=transform[\"test\"])\n\nbatch_size = 100 downloading the data set from datasets module train_data = datasets.CIFAR10(‘data’, train=True, download=True, transform=transform) test_data = datasets.CIFAR10(‘data’, train=False, download=True, transform=transform)\ndev_size = 0.2 Using a validation size of 20%, define the training and validation sampler that will be used to divide the dataset into those two sets. idx = list(range(len(train_data))) np.random.shuffle(idx) split_size = int(np.floor(dev_size * len(train_data))) train_idx, dev_idx = idx[split_size:], idx[:split_size]\n\ntrain_sampler = SubsetRandomSampler(train_idx)\ndev_sampler = SubsetRandomSampler(dev_idx)\nThe SubsetRandomSampler() function from pytorch is used to divide the original training set into training and validations by randomly sampling indexes.\n\n\ndev_size = 0.2\nidx = list(range(len(train_data)))\nnp.random.shuffle(idx)\nsplit_size = int(np.floor(dev_size * len(train_data)))\ntrain_idx, dev_idx = idx[split_size:], idx[:split_size]\n\ntrain_sampler = SubsetRandomSampler(train_idx)\ndev_sampler = SubsetRandomSampler(dev_idx)\n\n\nThe DataLoader() functions are the ones in charge of loading the images by batches.The resulting variables(train_loader,dev_loader,and test_loader) of this function will contain the values for the features and the target separately.\n\n\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\ndev_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=dev_sampler)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n\n\n\nDefine the architecture of your network. Use the following information to do so:\n• Conv1: A convolutional layer that takes as input the colored image and passes it through 10 filters of size 3. Both the padding and the stride should be set to 1.\n• Conv2: A convolutional layer that passes the input data through 20 filters of size 3. Both the padding and the stride should be set to 1.\n• Conv3: A convolutional layer that passes the input data through 40 filters of size three. Both the padding and the stride should be set to 1.\n• Use the ReLU activation function after each convolutional layer.\n• A pooling layer after each convolutional layer, with a filter size and stride of 2.\n• A dropout term set to 20% after flattening the image.\n• Linear1: A fully-connected layer that receives as input the flattened matrix from the previous layer and generates an output of 100 units. Use the ReLU activation function for this layer. A dropout term here is set to 20%.\n• Linear2: A fully-connected layer that generates 10 outputs, one for each class label. Use the log_softmax activation function for the output layer.\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 10, 3, 1, 1)\n        self.norm1 = nn.BatchNorm2d(10)\n        self.conv2 = nn.Conv2d(10, 20, 3, 1, 1)\n        self.norm2 = nn.BatchNorm2d(20)\n        self.conv3 = nn.Conv2d(20, 40, 3, 1, 1)\n        self.norm3 = nn.BatchNorm2d(40)\n        self.pool = nn.MaxPool2d(2, 2)\n        \n        self.linear1 = nn.Linear(40 * 4 * 4, 100)\n        self.norm4 = nn.BatchNorm1d(100)\n        self.linear2 = nn.Linear(100, 10)\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, x):\n        x = self.pool(self.norm1(F.relu(self.conv1(x))))\n        x = self.pool(self.norm2(F.relu(self.conv2(x))))\n        x = self.pool(self.norm3(F.relu(self.conv3(x))))\n\n        x = x.view(-1, 40 * 4 * 4)\n        x = self.dropout(x)\n        x = self.norm4(F.relu(self.linear1(x)))\n        x = self.dropout(x)\n        x = F.log_softmax(self.linear2(x), dim=1)\n        \n        return x\n\n\nmodel = CNN()\nloss_function = nn.NLLLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nepochs = 100\n\n\nTrain your network and be sure to save the values for the loss and accuracy of both the training and validation sets.\n\n\ntrain_losses, dev_losses, train_acc, dev_acc= [], [], [], []\nx_axis = []\n\nfor e in range(1, epochs+1):\n    losses = 0\n    acc = 0\n    iterations = 0\n    \n    model.train()\n    for data, target in train_loader:\n        iterations += 1\n\n        pred = model(data)\n        loss = loss_function(pred, target)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        losses += loss.item()\n        p = torch.exp(pred)\n        top_p, top_class = p.topk(1, dim=1)\n        acc += accuracy_score(target, top_class)\n        \n    dev_losss = 0\n    dev_accs = 0\n    iter_2 = 0\n        \n    if e%5 == 0 or e == 1:\n        x_axis.append(e)\n        \n        with torch.no_grad():\n            model.eval()\n            \n            for data_dev, target_dev in dev_loader:\n                iter_2 += 1\n                \n                dev_pred = model(data_dev)\n                dev_loss = loss_function(dev_pred, target_dev)\n                dev_losss += dev_loss.item()\n\n                dev_p = torch.exp(dev_pred)\n                top_p, dev_top_class = dev_p.topk(1, dim=1)\n                dev_accs += accuracy_score(target_dev, dev_top_class)\n        \n        train_losses.append(losses/iterations)\n        dev_losses.append(dev_losss/iter_2)\n        train_acc.append(acc/iterations)\n        dev_acc.append(dev_accs/iter_2)\n        \n        print(\"Epoch: {}/{}.. \".format(e, epochs),\n              \"Training Loss: {:.3f}.. \".format(losses/iterations),\n              \"Validation Loss: {:.3f}.. \".format(dev_losss/iter_2),\n              \"Training Accuracy: {:.3f}.. \".format(acc/iterations),\n              \"Validation Accuracy: {:.3f}\".format(dev_accs/iter_2))\n\n\nPlot the loss and accuracy of both sets\n\n\nplt.plot(x_axis,train_losses, label='Training loss')\nplt.plot(x_axis, dev_losses, label='Validation loss')\nplt.legend(frameon=False)\nplt.show()\n\n\nplt.plot(x_axis, train_acc, label=\"Training accuracy\")\nplt.plot(x_axis, dev_acc, label=\"Validation accuracy\")\nplt.legend(frameon=False)\nplt.show()\n\n\nmodel.eval()\niter_3 = 0\nacc_test = 0\nfor data_test, target_test in test_loader:\n    iter_3 += 1\n    test_pred = model(data_test)\n    test_pred = torch.exp(test_pred)\n    top_p, top_class_test = test_pred.topk(1, dim=1)\n    acc_test += accuracy_score(target_test, top_class_test)\nprint(acc_test/iter_3)\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/style_transfer_with_pytorch/index.html",
    "href": "posts/style_transfer_with_pytorch/index.html",
    "title": "Style_transfer_with_pytorch",
    "section": "",
    "text": "in simple words, style transfer consists of modifying the style of an image,whilte still preserving its content. for instance taking an image of an animal and transforming the style into a van Goh like painting.\n\n\n\nfeeding the inputs: Both the content and the style image are fed to the model,and they need to be the same. A common practice here is to resize the style image to be the same shape of the content image\nLoading the model: try model like VGG network,which performs outstandingly well over style transfer problems.\nDetermining the layers’functions: Given that there are two main tasks at hand (recognizing the content of an image and distinguishing the style of another one ),different layers will have different functions to extract the different features.for the style image ,the focus should be on colors and textures,while for the content image,the focus should be on edges and forms. in this step ,the different layers are separated into different tasks.\nDefining the optimization problem: Unlike other supervised problems. it is required to minimize three different loss functions for style transfer problems:\n\n\nContent loss: this measures the distance between the content image and the output,only considering features related to content\nStyle loss: this measures the distance between the style image and the output only considering features related to style\nTotal loss: This combines both the content and style loss. Both the content and style loss have a weight associated to them, which is used to determine their participation in the calculation of the total loss.\n\n\nParameter update: This step uses gradients to update the different parameters of the network\n\n\nimport numpy as np ## this will be used to transform images to be displayed\nimport torch \nfrom torch import nn, optim #These will implement the neural network as we as define the optimization algorithm \nfrom PIL import Image # This will load images \nimport matplotlib.pyplot as plt # this will display images \nfrom torchvision import transforms, models # These will convert the images into tensors and load the pretrained model \ndevice = 'cuda'\n\n\nimsize = 224 # set the image size  for both images \n# set the transformations to be performed over images --> resize--> converting to tensors --> normalizing \nloader = transforms.Compose([\n    transforms.Resize(imsize), \n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n\nNote:The VGG network was trained using normalized images, where each channel has a mean of 0.485, 0.456, and 0.406, respectively, and a standard deviation of 0.229, 0.224, and 0.225.\n\ndef image_loader(image_name):\n    \"\"\"\n    Function to receive the image path as input and use PIL to \n    open the image and apply transformations over the image \n    \"\"\"\n    image = Image.open(image_name)\n    image = loader(image).unsqueeze(0)\n    return image\n\n\ncontent_img = image_loader(\"images/landscape.jpg\").to(device)\nstyle_img = image_loader(\"images/monet.jpg\").to(device)\n\n\nTo display the images,convert them back to PIL images and revert the normalization process, Define these transformation in a variable\nNote: To revert the normalization, it is necessary to use as mean the negative value of the mean used for normalizing the data, divided by the standard deviation previously used for normalizing the data. Moreover, the new standard deviation should be equal to one divided by the standard deviation used to normalize the data before.\n\n\nunloader = transforms.Compose([\n    transforms.Normalize((-0.485/0.229, -0.456/0.224, -0.406/0.225), (1/0.229, 1/0.224, 1/0.225)),\n    transforms.ToPILImage()])\n\n\ndef tensor2image(tensor):\n    image = tensor.to('cpu').clone() \n    image = image.squeeze(0)  \n    image = unloader(image)\n    return image\n\n\nplt.figure()\nplt.imshow(tensor2image(content_img))\nplt.title(\"Content Image\")\nplt.show()\n\nplt.figure()\nplt.imshow(tensor2image(style_img))\nplt.title(\"Style Image\")\nplt.show()"
  },
  {
    "objectID": "posts/style_transfer_with_pytorch/index.html#extracting-the-features",
    "href": "posts/style_transfer_with_pytorch/index.html#extracting-the-features",
    "title": "Style_transfer_with_pytorch",
    "section": "Extracting the features",
    "text": "Extracting the features\n\nIn the field of style transfer, there have been different papers that have identified those layers that are crucial at recognizing relevant features over the content and style images.\nAccording to this, it is conventionally accepted that the first convolutional layer of every stack is capable of extracting style features, while only the second convolutional layer of the fourth stack should be used to extract content features.\nFrom now on, we will refer to the layers that extract the style features as conv1_1, conv2_1,conv3_1, conv4_1, and conv5_1, while the layer in charge of extracting the content features will be known as conv4_2.\nthis means that the sytle image should be passed through five different layers. while the content image only needs to go through one layer.\nthe output from each of these layers is used to compare the output image to the input images,where the objective would be to modify the parameters of the target image to resemble the content of the content image and the style of the style image,which can be achieved through the optimization of three different loss functions.\nto check the style representation of the target image and the style image, it is necessary to check for correlations and not the strict presence of the features on both images. this is because the style features of both images will not be exact,but rather an approximation."
  },
  {
    "objectID": "posts/style_transfer_with_pytorch/index.html#the-gram-matrix",
    "href": "posts/style_transfer_with_pytorch/index.html#the-gram-matrix",
    "title": "Style_transfer_with_pytorch",
    "section": "The gram matrix",
    "text": "The gram matrix\n[[https://www.youtube.com/watch?v=Elxnzxk-AUk]]"
  }
]